import re
import json
from engine.llm_client import chat
from typing import Set, Dict, List, Tuple

def normalize_answer_for_comparison(text: str) -> str:
    """
    Normalize answer text for comparison by removing trailing punctuation
    and standardizing whitespace.
    
    Args:
        text: The text to normalize
        
    Returns:
        Normalized text for comparison
    """
    if not isinstance(text, str):
        return str(text)
    
    # Remove leading/trailing whitespace
    normalized = text.strip()
    
    # Remove trailing punctuation (but preserve punctuation within the text)
    # Common punctuation that might appear at the end of Korean sentences
    trailing_punct = ['!', '?', '.', '„ÄÇ', 'ÔºÅ', 'Ôºü']
    
    # Keep removing trailing punctuation until none left
    while normalized and any(normalized.endswith(punct) for punct in trailing_punct):
        for punct in trailing_punct:
            if normalized.endswith(punct):
                normalized = normalized[:-len(punct)].strip()
                break
    
    # Normalize internal whitespace
    normalized = ' '.join(normalized.split())
    
    return normalized

def normalize_grammar_id(raw_id: str) -> str:
    """
    Enhanced normalization that creates consistent grammar IDs.
    
    Standard format: -{korean_pattern} or -{korean_pattern1}_{korean_pattern2}
    
    Rules:
    1. Always start with '-' prefix for grammar patterns
    2. Use '_' to separate alternative forms (not '/')
    3. Keep Korean characters intact
    4. Handle complex patterns systematically
    5. Remove extra spaces and punctuation
    
    Examples:
    'Ïù¥ÏóêÏöî/ÏòàÏöî' -> '-Ïù¥ÏóêÏöî_ÏòàÏöî'
    '-ÏïÑÏöî/-Ïñ¥Ïöî' -> '-ÏïÑÏöî_Ïñ¥Ïöî'
    'ÏùÄÎäî' -> '-ÏùÄ_Îäî'
    'topic marking particle' -> '-topic_marking_particle' (for English descriptions)
    """
    if not raw_id:
        return raw_id
    
    s = raw_id.strip()
    
    # Handle edge case: if it's purely an English description, convert to snake_case with prefix
    if re.match(r'^[a-zA-Z\s]+$', s):
        s = re.sub(r'\s+', '_', s.lower())
        return f'-{s}' if not s.startswith('-') else s
    
    # Comprehensive Korean grammar pattern detection and separation
    korean_patterns = {
        # Verb endings
        'Ïù¥ÏóêÏöîÏòàÏöî': 'Ïù¥ÏóêÏöî_ÏòàÏöî',
        'ÏïÑÏöîÏñ¥Ïöî': 'ÏïÑÏöî_Ïñ¥Ïöî',
        'ÏïòÏñ¥ÏöîÏóàÏñ¥Ïöî': 'ÏïòÏñ¥Ïöî_ÏóàÏñ¥Ïöî',
        'Ïúº„ÑπÍ±∞ÏòàÏöî': 'Ïúº„Ñπ_Í±∞ÏòàÏöî',
        'ÏïÑÏñ¥ÏïºÌï¥Ïöî': 'ÏïÑ_Ïñ¥Ïïº_Ìï¥Ïöî',
        'ÏïÑÏñ¥ÏÑú': 'ÏïÑ_Ïñ¥ÏÑú',
        'ÏïÑÏñ¥Î≥¥Îã§': 'ÏïÑ_Ïñ¥_Î≥¥Îã§',
        'ÏïÑÏñ¥Ï£ºÏÑ∏Ïöî': 'ÏïÑ_Ïñ¥_Ï£ºÏÑ∏Ïöî',
        'ÏïÑÏñ¥ÎèÑÎèºÏöî': 'ÏïÑ_Ïñ¥ÎèÑ_ÎèºÏöî',
        
        # Particles
        'ÏùÄÎäî': 'ÏùÄ_Îäî',
        'Ïù¥Í∞Ä': 'Ïù¥_Í∞Ä',
        'ÏùÑÎ•º': 'ÏùÑ_Î•º',
        
        # Complex patterns
        'Ïúº„Ñ¥ÎäîÍ≤ÉÍ∞ôÎã§': 'Ïúº„Ñ¥Îäî_Í≤É_Í∞ôÎã§',
        'Ïúº„ÑπÏàòÏûàÎã§ÏóÜÎã§': 'Ïúº„Ñπ_Ïàò_ÏûàÎã§_ÏóÜÎã§',
        'Í∏∞ÎïåÎ¨∏Ïóê': 'Í∏∞_ÎïåÎ¨∏Ïóê',
        'ÏúºÎ†§Í≥†ÌïòÎã§': 'ÏúºÎ†§Í≥†_ÌïòÎã§',
        'Í≥†ÎÇòÏÑú': 'Í≥†_ÎÇòÏÑú',
        'Ïúº„Ñ¥Ï†ÅÏù¥ÏûàÎã§': 'Ïúº„Ñ¥_Ï†ÅÏù¥_ÏûàÎã§',
        'Ïúº„Ñ¥Îã§ÏùåÏóê': 'Ïúº„Ñ¥_Îã§ÏùåÏóê',
        'Í∏∞Ï†ÑÏóê': 'Í∏∞_Ï†ÑÏóê',
        'Ïúº„ÑπÎïå': 'Ïúº„Ñπ_Îïå',
        'ÎäîÍ≤É': 'Îäî_Í≤É',
        'ÏùÄÎäîÍ≤É': 'ÏùÄ_Îäî_Í≤É',
        'Í≥†ÏûàÎã§': 'Í≥†_ÏûàÎã§',
        'Í≥†Ïã∂Îã§': 'Í≥†_Ïã∂Îã§',
        'ÏßÄÏïäÏïÑÏöî': 'ÏßÄ_ÏïäÏïÑÏöî',
        'ÏßÄÎ™ªÌïòÎã§': 'ÏßÄ_Î™ªÌïòÎã§',
        'ÏßÄÎßàÏÑ∏Ïöî': 'ÏßÄ_ÎßàÏÑ∏Ïöî',
        'Î∞ñÏóêÏóÜÎã§': 'Î∞ñÏóê_ÏóÜÎã§',
        'ÏúºÎ°úÍ∞ÄÎã§Ïò§Îã§': 'ÏúºÎ°ú_Í∞ÄÎã§_Ïò§Îã§',
    }
    
    # Apply Korean pattern corrections first
    for original, replacement in korean_patterns.items():
        s = s.replace(original, replacement)
    
    # Handle slash and dash separators systematically
    # Pattern: {prefix}korean{separator}korean{suffix}
    s = re.sub(r'([Í∞Ä-Ìû£]+)/([Í∞Ä-Ìû£]+)', r'\1_\2', s)  # ÏïÑÏöî/Ïñ¥Ïöî -> ÏïÑÏöî_Ïñ¥Ïöî
    s = re.sub(r'([Í∞Ä-Ìû£]+)-([Í∞Ä-Ìû£]+)', r'\1_\2', s)   # ÏïÑÏöî-Ïñ¥Ïöî -> ÏïÑÏöî_Ïñ¥Ïöî
    
    # Handle prefixed patterns: -korean/-korean
    s = re.sub(r'(-[Í∞Ä-Ìû£]+)/(-[Í∞Ä-Ìû£]+)', r'\1_\2', s)  # -ÏïÑÏöî/-Ïñ¥Ïöî -> -ÏïÑÏöî_-Ïñ¥Ïöî
    s = re.sub(r'(-[Í∞Ä-Ìû£]+)-(-[Í∞Ä-Ìû£]+)', r'\1_\2', s)   # -ÏïÑÏöî--Ïñ¥Ïöî -> -ÏïÑÏöî_-Ïñ¥Ïöî
    
    # Clean up double prefixes: -ÏïÑÏöî_-Ïñ¥Ïöî -> -ÏïÑÏöî_Ïñ¥Ïöî
    s = re.sub(r'_-([Í∞Ä-Ìû£])', r'_\1', s)
    
    # Ensure consistent prefix for Korean grammar patterns
    if re.search(r'[Í∞Ä-Ìû£]', s):  # Contains Korean characters
        if not s.startswith('-'):
            s = '-' + s
    
    # Normalize English parts to lowercase
    s = re.sub(r'[A-Z]', lambda m: m.group().lower(), s)
    
    # Clean up punctuation but preserve essential markers
    s = re.sub(r'[^\w\sÍ∞Ä-Ìû£_\-]', '', s)  # Keep Korean, word chars, spaces, underscores, hyphens
    s = re.sub(r'\s+', '_', s)  # Spaces to underscores
    s = re.sub(r'_+', '_', s)   # Collapse multiple underscores
    s = s.strip('_')            # Remove leading/trailing underscores
    
    return s

def merge_error_categories(existing, new):
    """
    Deterministically merge LLM-categorized session errors into existing categories.
    - LLM input is session-only: each 'new_cat' count is for this session.
    - Sum counts across sessions.
    - Merge examples, avoiding duplicates.
    """
    label_to_cat = {cat["label"]: cat for cat in existing}

    for new_cat in new:
        label = new_cat["label"]
        session_count = new_cat.get("count", 0)
        session_examples = set(new_cat.get("examples", []))

        if label in label_to_cat:
            existing_cat = label_to_cat[label]
            existing_cat["count"] = existing_cat.get("count", 0) + session_count
            combined = set(existing_cat.get("examples", [])) | session_examples
            existing_cat["examples"] = sorted(combined)
        else:
            # Ensure only session-level count is kept
            new_cat["count"] = session_count
            new_cat["examples"] = sorted(session_examples)
            existing.append(new_cat)

    return existing

def categorize_session_errors(session_errors, existing_categories, grammar_tree):
    """
    Ask the LLM to place new session errors into existing categories (or make new ones).
    """

    if not session_errors:
        return existing_categories

    examples = "\n".join(f"- {k} (x{v})" for k, v in sorted(session_errors.items(), key=lambda x: -x[1]))
    existing = json.dumps(existing_categories, indent=2, ensure_ascii=False)
    grammar_tree_json = json.dumps(grammar_tree, indent=2, ensure_ascii=False)

    prompt = f"""
You are a Korean language tutor assistant. Below are errors made by a student during today's session.
Please help organize them by:
- Matching each to an existing category if possible (based on examples or label).
- Creating a new category only if no good match exists.

## Today's Errors:
{examples}

## Existing Category Labels Only:
{[cat['label'] for cat in existing_categories]}

## Curriculum Grammar Points (for reference ‚Äî not mandatory):
{grammar_tree_json}

Return the updated full category list in this format:
{{
  "categories": [
    {{
      "label": "Politeness mismatch",
      "count": 4,
      "examples": ["used plain instead of polite", "missing Ïöî-ending"]
    }},
    ...
  ]
}}
    """

    response = sanitize_json_string(chat([{"role": "user", "content": prompt}], temperature=0.2))
    try:
        return json.loads(response)["categories"]
    except Exception as e:
        print("‚ö†Ô∏è Failed to parse session error categorization response:", e)
        return existing_categories

def summarize_common_errors(error_dict):
    """
    Send the full common_errors dictionary to the LLM and receive grouped categories.
    Each category includes a label, frequency, and representative examples.
    """

    if not error_dict:
        return []

    # Build examples string for prompt
    examples = "\n".join(f"- {k} (x{v})" for k, v in sorted(error_dict.items(), key=lambda x: -x[1]))

    prompt = f"""
You are an intelligent assistant helping analyze Korean learner errors. Below is a list of raw error messages collected from their recent sessions, along with how many times each occurred.

Be sure to genralize! This list is important and will be used for further teaching of the user!

Your job is to:
- Group similar error messages into clear categories.
- For each category, provide:
  - A short label (in English)
  - Total frequency (sum of errors in this group)
  - A few representative examples

Here are the raw errors:
{examples}

Return JSON in this format:
{{
  "categories": [
    {{
      "label": "Politeness mismatch",
      "count": 7,
      "examples": ["used plain instead of polite verb", "..."]
    }},
    ...
  ]
}}
"""
    print(f' ==> [Line 62]: \033[38;2;234;242;89m[prompt]\033[0m({type(prompt).__name__}) = \033[38;2;59;236;212m{prompt}\033[0m')

    response = sanitize_json_string(chat([{"role": "user", "content": prompt}], temperature=0.2))
    print(f' ==> [Line 64]: \033[38;2;223;121;144m[response]\033[0m({type(response).__name__}) = \033[38;2;254;2;107m{response}\033[0m')

    try:
        return json.loads(response)["categories"]
    except Exception as e:
        print("‚ö†Ô∏è Could not parse LLM response:", e)
        return []

def sanitize_json_string(s):
    s = s.strip()

    # Remove <think>...</think> and anything before first {
    s = re.sub(r"<think>.*?</think>", "", s, flags=re.DOTALL)
    
    # Keep only content between first "{" and last "}"
    if "{" in s and "}" in s:
        start = s.find("{")
        end = s.rfind("}")
        s = s[start:end+1]

    s = s.replace('"', '"').replace('"', '"').replace("'", "'")
    return s

def migrate_grammar_profile(profile: dict) -> Tuple[dict, List[str]]:
    """
    Migrate existing profile to use consistent grammar IDs.
    
    Returns:
        tuple: (updated_profile, migration_log)
    """
    migration_log = []
    
    if 'grammar_summary' not in profile:
        return profile, migration_log
    
    old_grammar_summary = profile['grammar_summary'].copy()
    new_grammar_summary = {}
    
    for old_id, data in old_grammar_summary.items():
        new_id = normalize_grammar_id(old_id)
        
        if old_id != new_id:
            migration_log.append(f"Migrated: '{old_id}' -> '{new_id}'")
            
            # If the new_id already exists, merge the data intelligently
            if new_id in new_grammar_summary:
                migration_log.append(f"Merging duplicate: '{new_id}'")
                # Keep the data with more repetitions (more advanced SRS state)
                existing_data = new_grammar_summary[new_id]
                if data.get('reps', 0) > existing_data.get('reps', 0):
                    new_grammar_summary[new_id] = data
                    migration_log.append(f"  Used data from '{old_id}' (more reps)")
                else:
                    migration_log.append(f"  Kept existing data (more reps)")
            else:
                new_grammar_summary[new_id] = data
        else:
            # ID was already normalized
            new_grammar_summary[new_id] = data
    
    profile['grammar_summary'] = new_grammar_summary
    return profile, migration_log

def validate_grammar_ids(grammar_ids: List[str]) -> Dict[str, List[str]]:
    """
    Validate a list of grammar IDs and report issues.
    
    Returns:
        dict: {
            'valid': [list of valid IDs],
            'invalid': [list of invalid IDs],
            'normalized': [list of suggested normalizations]
        }
    """
    valid = []
    invalid = []
    normalized = []
    
    for gid in grammar_ids:
        if not gid or not isinstance(gid, str):
            invalid.append(f"Invalid type: {repr(gid)}")
            continue
            
        normalized_id = normalize_grammar_id(gid)
        
        if gid != normalized_id:
            normalized.append(f"'{gid}' -> '{normalized_id}'")
        
        # Check if it follows our standard pattern
        if re.match(r'^-[Í∞Ä-Ìû£_]+$', normalized_id) or re.match(r'^-[a-z_]+$', normalized_id):
            valid.append(normalized_id)
        else:
            invalid.append(f"Doesn't match standard pattern: '{normalized_id}'")
    
    return {
        'valid': valid,
        'invalid': invalid,
        'normalized': normalized
    }

def find_grammar_duplicates(profile: dict) -> List[Tuple[str, str]]:
    """
    Find potential duplicate grammar IDs that should be the same.
    
    Returns:
        List of tuples: [(id1, id2), ...] where id1 and id2 are likely duplicates
    """
    grammar_summary = profile.get('grammar_summary', {})
    ids = list(grammar_summary.keys())
    
    # Normalize all IDs and group by normalized form
    normalized_groups = {}
    for gid in ids:
        normalized = normalize_grammar_id(gid)
        if normalized not in normalized_groups:
            normalized_groups[normalized] = []
        normalized_groups[normalized].append(gid)
    
    # Find groups with multiple original IDs
    duplicates = []
    for normalized, original_ids in normalized_groups.items():
        if len(original_ids) > 1:
            # Return all pairs in this group
            for i in range(len(original_ids)):
                for j in range(i + 1, len(original_ids)):
                    duplicates.append((original_ids[i], original_ids[j]))
    
    return duplicates

def test_normalization():
    """
    Test the normalization function with various inputs.
    """
    test_cases = [
        # Basic patterns
        ('-Ïù¥ÏóêÏöî/ÏòàÏöî', '-Ïù¥ÏóêÏöî_ÏòàÏöî'),
        ('-ÏïÑÏöî/-Ïñ¥Ïöî', '-ÏïÑÏöî_Ïñ¥Ïöî'),
        ('ÏùÄÎäî', '-ÏùÄ_Îäî'),
        ('Ïù¥Í∞Ä', '-Ïù¥_Í∞Ä'),
        ('ÏùÑÎ•º', '-ÏùÑ_Î•º'),
        
        # Complex patterns
        ('-ÏïòÏñ¥Ïöî/-ÏóàÏñ¥Ïöî', '-ÏïòÏñ¥Ïöî_ÏóàÏñ¥Ïöî'),
        ('Ïúº„ÑπÍ±∞ÏòàÏöî', '-Ïúº„Ñπ_Í±∞ÏòàÏöî'),
        ('ÏïÑÏöîÏñ¥Ïöî', '-ÏïÑÏöî_Ïñ¥Ïöî'),
        
        # Edge cases
        ('', ''),
        ('topic marking particle', '-topic_marking_particle'),
        ('-ÏßÄ_ÏïäÏïÑÏöî', '-ÏßÄ_ÏïäÏïÑÏöî'),  # Already normalized
        
        # Mixed formats
        ('Ïù¥ÏóêÏöîÏòàÏöî', '-Ïù¥ÏóêÏöî_ÏòàÏöî'),
        ('-Ïúº„Ñ¥ÎäîÍ≤ÉÍ∞ôÎã§', '-Ïúº„Ñ¥Îäî_Í≤É_Í∞ôÎã§'),
    ]
    
    print("üß™ Testing grammar ID normalization...")
    all_passed = True
    
    for input_id, expected in test_cases:
        result = normalize_grammar_id(input_id)
        if result == expected:
            print(f"‚úÖ '{input_id}' -> '{result}'")
        else:
            print(f"‚ùå '{input_id}' -> '{result}' (expected '{expected}')")
            all_passed = False
    
    if all_passed:
        print("üéâ All tests passed!")
    else:
        print("‚ö†Ô∏è Some tests failed.")
    
    return all_passed

# Additional utility: Curriculum consistency checker
def check_curriculum_consistency(curriculum_path: str = None) -> Dict[str, any]:
    """
    Check if curriculum grammar IDs are consistently normalized.
    """
    if not curriculum_path:
        curriculum_path = 'curriculum/korean.json'
    
    try:
        with open(curriculum_path, 'r', encoding='utf-8') as f:
            curriculum = json.load(f)
    except FileNotFoundError:
        return {'error': f'Curriculum file not found: {curriculum_path}'}
    
    grammar_points = curriculum.get('grammar_points', [])
    all_ids = [gp.get('id', '') for gp in grammar_points]
    
    validation_result = validate_grammar_ids(all_ids)
    
    return {
        'total_grammar_points': len(grammar_points),
        'validation': validation_result,
        'needs_migration': len(validation_result['normalized']) > 0
    }

if __name__ == '__main__':
    # Run tests
    test_normalization()
    
    # Check curriculum consistency
    print("\nüìö Checking curriculum consistency...")
    curriculum_check = check_curriculum_consistency()
    if 'error' not in curriculum_check:
        print(f"Total grammar points: {curriculum_check['total_grammar_points']}")
        print(f"Needs migration: {curriculum_check['needs_migration']}")
        if curriculum_check['validation']['normalized']:
            print("Suggested normalizations:")
            for norm in curriculum_check['validation']['normalized']:
                print(f"  {norm}")
    else:
        print(f"Error: {curriculum_check['error']}")