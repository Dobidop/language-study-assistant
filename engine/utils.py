import re
import json
from engine.llm_client import chat
from typing import Set, Dict, List, Tuple

def normalize_answer_for_comparison(text: str) -> str:
    """
    Normalize answer text for comparison by removing trailing punctuation
    and standardizing whitespace.
    
    Args:
        text: The text to normalize
        
    Returns:
        Normalized text for comparison
    """
    if not isinstance(text, str):
        return str(text)
    
    # Remove leading/trailing whitespace
    normalized = text.strip()
    
    # Remove trailing punctuation (but preserve punctuation within the text)
    # Common punctuation that might appear at the end of Korean sentences
    trailing_punct = ['!', '?', '.', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    
    # Keep removing trailing punctuation until none left
    while normalized and any(normalized.endswith(punct) for punct in trailing_punct):
        for punct in trailing_punct:
            if normalized.endswith(punct):
                normalized = normalized[:-len(punct)].strip()
                break
    
    # Normalize internal whitespace
    normalized = ' '.join(normalized.split())
    
    return normalized

def normalize_grammar_id(raw_id: str) -> str:
    """
    Enhanced normalization that creates consistent grammar IDs.
    
    Standard format: -{korean_pattern} or -{korean_pattern1}_{korean_pattern2}
    
    Rules:
    1. Always start with '-' prefix for grammar patterns
    2. Use '_' to separate alternative forms (not '/')
    3. Keep Korean characters intact
    4. Handle complex patterns systematically
    5. Remove extra spaces and punctuation
    
    Examples:
    'ì´ì—ìš”/ì˜ˆìš”' -> '-ì´ì—ìš”_ì˜ˆìš”'
    '-ì•„ìš”/-ì–´ìš”' -> '-ì•„ìš”_ì–´ìš”'
    'ì€ëŠ”' -> '-ì€_ëŠ”'
    'topic marking particle' -> '-topic_marking_particle' (for English descriptions)
    """
    if not raw_id:
        return raw_id
    
    s = raw_id.strip()
    
    # Handle edge case: if it's purely an English description, convert to snake_case with prefix
    if re.match(r'^[a-zA-Z\s]+$', s):
        s = re.sub(r'\s+', '_', s.lower())
        return f'-{s}' if not s.startswith('-') else s
    
    # Comprehensive Korean grammar pattern detection and separation
    korean_patterns = {
        # Verb endings
        'ì´ì—ìš”ì˜ˆìš”': 'ì´ì—ìš”_ì˜ˆìš”',
        'ì•„ìš”ì–´ìš”': 'ì•„ìš”_ì–´ìš”',
        'ì•˜ì–´ìš”ì—ˆì–´ìš”': 'ì•˜ì–´ìš”_ì—ˆì–´ìš”',
        'ìœ¼ã„¹ê±°ì˜ˆìš”': 'ìœ¼ã„¹_ê±°ì˜ˆìš”',
        'ì•„ì–´ì•¼í•´ìš”': 'ì•„_ì–´ì•¼_í•´ìš”',
        'ì•„ì–´ì„œ': 'ì•„_ì–´ì„œ',
        'ì•„ì–´ë³´ë‹¤': 'ì•„_ì–´_ë³´ë‹¤',
        'ì•„ì–´ì£¼ì„¸ìš”': 'ì•„_ì–´_ì£¼ì„¸ìš”',
        'ì•„ì–´ë„ë¼ìš”': 'ì•„_ì–´ë„_ë¼ìš”',
        
        # Particles
        'ì€ëŠ”': 'ì€_ëŠ”',
        'ì´ê°€': 'ì´_ê°€',
        'ì„ë¥¼': 'ì„_ë¥¼',
        
        # Complex patterns
        'ìœ¼ã„´ëŠ”ê²ƒê°™ë‹¤': 'ìœ¼ã„´ëŠ”_ê²ƒ_ê°™ë‹¤',
        'ìœ¼ã„¹ìˆ˜ìžˆë‹¤ì—†ë‹¤': 'ìœ¼ã„¹_ìˆ˜_ìžˆë‹¤_ì—†ë‹¤',
        'ê¸°ë•Œë¬¸ì—': 'ê¸°_ë•Œë¬¸ì—',
        'ìœ¼ë ¤ê³ í•˜ë‹¤': 'ìœ¼ë ¤ê³ _í•˜ë‹¤',
        'ê³ ë‚˜ì„œ': 'ê³ _ë‚˜ì„œ',
        'ìœ¼ã„´ì ì´ìžˆë‹¤': 'ìœ¼ã„´_ì ì´_ìžˆë‹¤',
        'ìœ¼ã„´ë‹¤ìŒì—': 'ìœ¼ã„´_ë‹¤ìŒì—',
        'ê¸°ì „ì—': 'ê¸°_ì „ì—',
        'ìœ¼ã„¹ë•Œ': 'ìœ¼ã„¹_ë•Œ',
        'ëŠ”ê²ƒ': 'ëŠ”_ê²ƒ',
        'ì€ëŠ”ê²ƒ': 'ì€_ëŠ”_ê²ƒ',
        'ê³ ìžˆë‹¤': 'ê³ _ìžˆë‹¤',
        'ê³ ì‹¶ë‹¤': 'ê³ _ì‹¶ë‹¤',
        'ì§€ì•Šì•„ìš”': 'ì§€_ì•Šì•„ìš”',
        'ì§€ëª»í•˜ë‹¤': 'ì§€_ëª»í•˜ë‹¤',
        'ì§€ë§ˆì„¸ìš”': 'ì§€_ë§ˆì„¸ìš”',
        'ë°–ì—ì—†ë‹¤': 'ë°–ì—_ì—†ë‹¤',
        'ìœ¼ë¡œê°€ë‹¤ì˜¤ë‹¤': 'ìœ¼ë¡œ_ê°€ë‹¤_ì˜¤ë‹¤',
    }
    
    # Apply Korean pattern corrections first
    for original, replacement in korean_patterns.items():
        s = s.replace(original, replacement)
    
    # Handle slash and dash separators systematically
    # Pattern: {prefix}korean{separator}korean{suffix}
    s = re.sub(r'([ê°€-íž£]+)/([ê°€-íž£]+)', r'\1_\2', s)  # ì•„ìš”/ì–´ìš” -> ì•„ìš”_ì–´ìš”
    s = re.sub(r'([ê°€-íž£]+)-([ê°€-íž£]+)', r'\1_\2', s)   # ì•„ìš”-ì–´ìš” -> ì•„ìš”_ì–´ìš”
    
    # Handle prefixed patterns: -korean/-korean
    s = re.sub(r'(-[ê°€-íž£]+)/(-[ê°€-íž£]+)', r'\1_\2', s)  # -ì•„ìš”/-ì–´ìš” -> -ì•„ìš”_-ì–´ìš”
    s = re.sub(r'(-[ê°€-íž£]+)-(-[ê°€-íž£]+)', r'\1_\2', s)   # -ì•„ìš”--ì–´ìš” -> -ì•„ìš”_-ì–´ìš”
    
    # Clean up double prefixes: -ì•„ìš”_-ì–´ìš” -> -ì•„ìš”_ì–´ìš”
    s = re.sub(r'_-([ê°€-íž£])', r'_\1', s)
    
    # Ensure consistent prefix for Korean grammar patterns
    if re.search(r'[ê°€-íž£]', s):  # Contains Korean characters
        if not s.startswith('-'):
            s = '-' + s
    
    # Normalize English parts to lowercase
    s = re.sub(r'[A-Z]', lambda m: m.group().lower(), s)
    
    # Clean up punctuation but preserve essential markers
    s = re.sub(r'[^\w\sê°€-íž£_\-]', '', s)  # Keep Korean, word chars, spaces, underscores, hyphens
    s = re.sub(r'\s+', '_', s)  # Spaces to underscores
    s = re.sub(r'_+', '_', s)   # Collapse multiple underscores
    s = s.strip('_')            # Remove leading/trailing underscores
    return s

def sanitize_json_string(s):
    s = s.strip()

    # Remove <think>...</think> and anything before first {
    s = re.sub(r"<think>.*?</think>", "", s, flags=re.DOTALL)
    
    # Keep only content between first "{" and last "}"
    if "{" in s and "}" in s:
        start = s.find("{")
        end = s.rfind("}")
        s = s[start:end+1]

    s = s.replace('"', '"').replace('"', '"').replace("'", "'")
    return s

def migrate_grammar_profile(profile: dict) -> Tuple[dict, List[str]]:
    """
    Migrate existing profile to use consistent grammar IDs.
    
    Returns:
        tuple: (updated_profile, migration_log)
    """
    migration_log = []
    
    if 'grammar_summary' not in profile:
        return profile, migration_log
    
    old_grammar_summary = profile['grammar_summary'].copy()
    new_grammar_summary = {}
    
    for old_id, data in old_grammar_summary.items():
        new_id = normalize_grammar_id(old_id)
        
        if old_id != new_id:
            migration_log.append(f"Migrated: '{old_id}' -> '{new_id}'")
            
            # If the new_id already exists, merge the data intelligently
            if new_id in new_grammar_summary:
                migration_log.append(f"Merging duplicate: '{new_id}'")
                # Keep the data with more repetitions (more advanced SRS state)
                existing_data = new_grammar_summary[new_id]
                if data.get('reps', 0) > existing_data.get('reps', 0):
                    new_grammar_summary[new_id] = data
                    migration_log.append(f"  Used data from '{old_id}' (more reps)")
                else:
                    migration_log.append(f"  Kept existing data (more reps)")
            else:
                new_grammar_summary[new_id] = data
        else:
            # ID was already normalized
            new_grammar_summary[new_id] = data
    
    profile['grammar_summary'] = new_grammar_summary
    return profile, migration_log

def validate_grammar_ids(grammar_ids: List[str]) -> Dict[str, List[str]]:
    """
    Validate a list of grammar IDs and report issues.
    
    Returns:
        dict: {
            'valid': [list of valid IDs],
            'invalid': [list of invalid IDs],
            'normalized': [list of suggested normalizations]
        }
    """
    valid = []
    invalid = []
    normalized = []
    
    for gid in grammar_ids:
        if not gid or not isinstance(gid, str):
            invalid.append(f"Invalid type: {repr(gid)}")
            continue
            
        normalized_id = normalize_grammar_id(gid)
        
        if gid != normalized_id:
            normalized.append(f"'{gid}' -> '{normalized_id}'")
        
        # Check if it follows our standard pattern
        if re.match(r'^-[ê°€-íž£_]+$', normalized_id) or re.match(r'^-[a-z_]+$', normalized_id):
            valid.append(normalized_id)
        else:
            invalid.append(f"Doesn't match standard pattern: '{normalized_id}'")
    
    return {
        'valid': valid,
        'invalid': invalid,
        'normalized': normalized
    }

def find_grammar_duplicates(profile: dict) -> List[Tuple[str, str]]:
    """
    Find potential duplicate grammar IDs that should be the same.
    
    Returns:
        List of tuples: [(id1, id2), ...] where id1 and id2 are likely duplicates
    """
    grammar_summary = profile.get('grammar_summary', {})
    ids = list(grammar_summary.keys())
    
    # Normalize all IDs and group by normalized form
    normalized_groups = {}
    for gid in ids:
        normalized = normalize_grammar_id(gid)
        if normalized not in normalized_groups:
            normalized_groups[normalized] = []
        normalized_groups[normalized].append(gid)
    
    # Find groups with multiple original IDs
    duplicates = []
    for normalized, original_ids in normalized_groups.items():
        if len(original_ids) > 1:
            # Return all pairs in this group
            for i in range(len(original_ids)):
                for j in range(i + 1, len(original_ids)):
                    duplicates.append((original_ids[i], original_ids[j]))
    
    return duplicates

def test_normalization():
    """
    Test the normalization function with various inputs.
    """
    test_cases = [
        # Basic patterns
        ('-ì´ì—ìš”/ì˜ˆìš”', '-ì´ì—ìš”_ì˜ˆìš”'),
        ('-ì•„ìš”/-ì–´ìš”', '-ì•„ìš”_ì–´ìš”'),
        ('ì€ëŠ”', '-ì€_ëŠ”'),
        ('ì´ê°€', '-ì´_ê°€'),
        ('ì„ë¥¼', '-ì„_ë¥¼'),
        
        # Complex patterns
        ('-ì•˜ì–´ìš”/-ì—ˆì–´ìš”', '-ì•˜ì–´ìš”_ì—ˆì–´ìš”'),
        ('ìœ¼ã„¹ê±°ì˜ˆìš”', '-ìœ¼ã„¹_ê±°ì˜ˆìš”'),
        ('ì•„ìš”ì–´ìš”', '-ì•„ìš”_ì–´ìš”'),
        
        # Edge cases
        ('', ''),
        ('topic marking particle', '-topic_marking_particle'),
        ('-ì§€_ì•Šì•„ìš”', '-ì§€_ì•Šì•„ìš”'),  # Already normalized
        
        # Mixed formats
        ('ì´ì—ìš”ì˜ˆìš”', '-ì´ì—ìš”_ì˜ˆìš”'),
        ('-ìœ¼ã„´ëŠ”ê²ƒê°™ë‹¤', '-ìœ¼ã„´ëŠ”_ê²ƒ_ê°™ë‹¤'),
    ]
    
    print("ðŸ§ª Testing grammar ID normalization...")
    all_passed = True
    
    for input_id, expected in test_cases:
        result = normalize_grammar_id(input_id)
        if result == expected:
            print(f"âœ… '{input_id}' -> '{result}'")
        else:
            print(f"âŒ '{input_id}' -> '{result}' (expected '{expected}')")
            all_passed = False
    
    if all_passed:
        print("ðŸŽ‰ All tests passed!")
    else:
        print("âš ï¸ Some tests failed.")
    
    return all_passed

# Additional utility: Curriculum consistency checker
def check_curriculum_consistency(curriculum_path: str = None) -> Dict[str, any]:
    """
    Check if curriculum grammar IDs are consistently normalized.
    """
    if not curriculum_path:
        curriculum_path = 'curriculum/korean.json'
    
    try:
        with open(curriculum_path, 'r', encoding='utf-8') as f:
            curriculum = json.load(f)
    except FileNotFoundError:
        return {'error': f'Curriculum file not found: {curriculum_path}'}
    
    grammar_points = curriculum.get('grammar_points', [])
    all_ids = [gp.get('id', '') for gp in grammar_points]
    
    validation_result = validate_grammar_ids(all_ids)
    
    return {
        'total_grammar_points': len(grammar_points),
        'validation': validation_result,
        'needs_migration': len(validation_result['normalized']) > 0
    }

if __name__ == '__main__':
    # Run tests
    test_normalization()
    
    # Check curriculum consistency
    print("\nðŸ“š Checking curriculum consistency...")
    curriculum_check = check_curriculum_consistency()
    if 'error' not in curriculum_check:
        print(f"Total grammar points: {curriculum_check['total_grammar_points']}")
        print(f"Needs migration: {curriculum_check['needs_migration']}")
        if curriculum_check['validation']['normalized']:
            print("Suggested normalizations:")
            for norm in curriculum_check['validation']['normalized']:
                print(f"  {norm}")
    else:
        print(f"Error: {curriculum_check['error']}")